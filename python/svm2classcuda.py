#!/usr/bin/env python

import uuid
import subprocess
import shlex
import csv
import sys
import time
from sklearn.base import BaseEstimator,RegressorMixin, ClassifierMixin
from sklearn.datasets import load_svmlight_file,dump_svmlight_file

import libsvm
import libsvm.svmutil
import numpy as np
import scipy.sparse
import scipy

path_to_train_program='../bin/linux/release/svm2classcudatrain'

class ClassifySVM2ClassCuda(BaseEstimator, ClassifierMixin):

    def __init__(self
                 ,base_str='/tmp/'+uuid.uuid4().hex
                 ,C=1
                 ,kernel='rbf'
                 ,gamma=1
                 #,tol=0.001
                 ,tol=0.1
                 ,max_iter=1000000
                 #,epsilon=1e-4
                 ,epsilon=1e-2
                 ):
        argdict= locals()
        argdict.pop('argdict',None)
        argdict.pop('self',None)
        vars(self).update(argdict)
##may work ?        
        t_str=''
        # if kernel=='linear':
        #     t_str='-t 0'
        # if kernel=='poly':
        #     t_str='-k polynomial'        
        #     self.param_str=' {0} -C {1} -1 {2}'.format(t_str,C,gamma)
        # elif kernel=='sigmoid':
        #     t_str='-k sigmoid'
        # else: # kernel=='rbf':
        
        t_str='--gaussian'
        #self.param_str=' {0} -c {1} -g {2} -t 0.01 -e 1e-4'.format(t_str,C,gamma)
        self.param_str=' {0} -c {1} -g {2} -t {3} -e {4}'.format(t_str,C,gamma,tol,epsilon)        

        self.predict_fname=' '
        self.test_fname=' '
        self.train_fname=' '
        self.model_fname=' '
        
        
    def __del__(self):
        p = subprocess.Popen(['rm',self.model_fname ,self.train_fname , self.test_fname ,  self.predict_fname], stdout=None,stderr=None)
        p.wait()

        
    def fit(self, X, Y):

        self.labels=list(set(Y))
        if len(self.labels) > 2 :
            self.multiclass=True
            #print 'multiclass'
        else:
            self.multiclass=False
            
        self.train_fname =self.base_str +'-svmcmd-train' +  '.dat'
        self.model_fname =self.train_fname + '.model'
        if isinstance(X,list):
            self.test_n_sample=len(X)
            X=np.array(X)
        else:
            self.test_n_sample=X.shape[0]
        dump_svmlight_file_dense(X,Y,self.train_fname ,zero_based=False)
        command_line='{0} {1} -o {3} {2}'.format(path_to_train_program,self.param_str, self.train_fname , self.model_fname )

        args = shlex.split(command_line)
        p = subprocess.Popen(args)
        p.wait()
        self.model=libsvm.svmutil.svm_load_model(self.model_fname)
        return self
    
    def predict(self, X):
        if isinstance(X,list):
            self.test_n_sample=len(X)
        else:
            self.test_n_sample=X.shape[0]
            X=X.tolist()
        Y=[1]*self.test_n_sample
        Y_predict_obj=libsvm.svmutil.svm_predict(Y,X,self.model)
        return np.array(map(int,Y_predict_obj[0]))
            


####copy from sklean dump_svmlight
from sklearn.externals.six import u, b
def _dump_svmlight_dense(X, y, f, one_based, comment, query_id):
    is_sp = int(hasattr(X, "tocsr"))
    if X.dtype.kind == 'i':
        value_pattern = u("%d:%d")
    else:
        value_pattern = u("%d:%.16g")

    if y.dtype.kind == 'i':
        line_pattern = u("%d")
    else:
        line_pattern = u("%.16g")

    if query_id is not None:
        line_pattern += u(" qid:%d")
    line_pattern += u(" %s\n")

    if comment:
        f.write(b("# Generated by dump_svmlight_file from scikit-learn %s\n"
                % __version__))
        f.write(b("# Column indices are %s-based\n"
                  % ["zero", "one"][one_based]))

        f.write(b("#\n"))
        f.writelines(b("# %s\n" % line) for line in comment.splitlines())

    for i in range(X.shape[0]):
        if is_sp:
            #print 'is_sp'
            span = slice(X.indptr[i], X.indptr[i + 1])
            row = zip(X.indices[span], X.data[span])
        else:
            #nz = X[i] != 0
            #row = zip(np.where(nz)[0], X[i, nz])
            row = [(j,X[i][j]) for j in range(len(X[i]))]
            #print row

        s = " ".join(value_pattern % (j + one_based, x) for j, x in row)
        if query_id is not None:
            feat = (y[i], query_id[i], s)
        else:
            feat = (y[i], s)
        f.write((line_pattern % feat).encode('ascii'))


def dump_svmlight_file_dense(X, y, f, zero_based=True, comment=None, query_id=None):
    """Dump the dataset in svmlight / libsvm file format.

    This format is a text-based format, with one sample per line. It does
    not store zero valued features hence is suitable for sparse dataset.

    The first element of each line can be used to store a target variable
    to predict.

    Parameters
    ----------
    X : {array-like, sparse matrix}, shape = [n_samples, n_features]
        Training vectors, where n_samples is the number of samples and
        n_features is the number of features.

    y : array-like, shape = [n_samples]
        Target values.

    f : string or file-like in binary mode
        If string, specifies the path that will contain the data.
        If file-like, data will be written to f. f should be opened in binary
        mode.

    zero_based : boolean, optional
        Whether column indices should be written zero-based (True) or one-based
        (False).

    comment : string, optional
        Comment to insert at the top of the file. This should be either a
        Unicode string, which will be encoded as UTF-8, or an ASCII byte
        string.
        If a comment is given, then it will be preceded by one that identifies
        the file as having been dumped by scikit-learn. Note that not all
        tools grok comments in SVMlight files.

    query_id : array-like, shape = [n_samples]
        Array containing pairwise preference constraints (qid in svmlight
        format).
    """
    if comment is not None:
        # Convert comment string to list of lines in UTF-8.
        # If a byte string is passed, then check whether it's ASCII;
        # if a user wants to get fancy, they'll have to decode themselves.
        # Avoid mention of str and unicode types for Python 3.x compat.
        if isinstance(comment, bytes):
            comment.decode("ascii")     # just for the exception
        else:
            comment = comment.encode("utf-8")
        if six.b("\0") in comment:
            raise ValueError("comment string contains NUL byte")

    y = np.asarray(y)
    if y.ndim != 1:
        raise ValueError("expected y of shape (n_samples,), got %r"
                         % (y.shape,))

    #### Xval = atleast2d_or_csr(X) ####always dense

    Xval=np.vstack(X)
    #print Xval.shape
    
    if Xval.shape[0] != y.shape[0]:
        raise ValueError("X.shape[0] and y.shape[0] should be the same, got"
                         " %r and %r instead." % (Xval.shape[0], y.shape[0]))

    # We had some issues with CSR matrices with unsorted indices (e.g. #1501),
    # so sort them here, but first make sure we don't modify the user's X.
    # TODO We can do this cheaper; sorted_indices copies the whole matrix.
    if Xval is X and hasattr(Xval, "sorted_indices"):
        X = Xval.sorted_indices()
    else:
        X = Xval
        if hasattr(X, "sort_indices"):
            X.sort_indices()

    if query_id is not None:
        query_id = np.asarray(query_id)
        if query_id.shape[0] != y.shape[0]:
            raise ValueError("expected query_id of shape (n_samples,), got %r"
                             % (query_id.shape,))

    one_based = not zero_based

    if hasattr(f, "write"):
        _dump_svmlight_dense(X, y, f, one_based, comment, query_id)
    else:
        with open(f, "wb") as f:
            _dump_svmlight_dense(X, y, f, one_based, comment, query_id)

    
from sklearn.datasets import load_svmlight_file,dump_svmlight_file
        
def cross(args):
    
    X,Y=load_svmlight_file(args.train_fname)
    X=X.toarray()
    from sklearn import cross_validation
    clfcmd=ClassifySVM2ClassCuda(C=args.c, gamma=args.g, kernel='rbf')
    scores = cross_validation.cross_val_score(clfcmd, X, Y, cv=args.v,n_jobs=1)
    time.sleep(1)
    print 'Cross Validation Accuracy = {0}%'.format(np.mean(scores)*100)


from argparse import ArgumentParser

def main():
    
    desc = u'{0} [Args] [Options]\n -h'.format(__file__)
    parser = ArgumentParser(description=desc)

    parser.add_argument('train_fname',  action='store',  nargs=None, const=None,  default=None, type=str, choices=None, help='train dat file', metavar=None)

    parser.add_argument('model_fname',  action='store',  nargs='?',  const=None,  default=None, type=str, choices=None, help='model file', metavar=None)

    parser.add_argument(
        '-v', 
        type = int,
        dest = 'v',
        #default = 5,
        help = 'cross '
    )

    parser.add_argument(
        '-c', 
        type = float,
        dest = 'c',
        default = 1,
        help = 'c'
    )

    parser.add_argument(
        '-g', 
        type = float,
        dest = 'g',
        #default = 5,
        help = 'g'
    )
    
    args = parser.parse_args()

    if args.v :
        cross(args)

    
if __name__ == '__main__':
    #print 'main'
    main()

    
    
